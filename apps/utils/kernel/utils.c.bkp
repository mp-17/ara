// The APACHE License (APACHE)

// Copyright (c) 2022 Xiaorui Yin. All rights reserved.

// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at

//   http://www.apache.org/licenses/LICENSE-2.0

// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include "riscv_vector.h"

void matadd(float *mat_a, float *mat_b, float *o, int row, int col) {
  size_t vlmax = vsetvlmax_e32m1();
  for (int i = 0; i < row; i++) {
    for (int j = 0; j < col;) {
      int vl = vlmax;
      if (j + vlmax > col) vl = vsetvl_e32m1(col - i);

      vfloat32m1_t a_vec = vle32_v_f32m1(&mat_a[i * col + j], vl);
      vfloat32m1_t b_vec = vle32_v_f32m1(&mat_b[i * col + j], vl);
      
      vfloat32m1_t o_vec = vfadd_vv_f32m1(a_vec, b_vec, vl);
      
      vse32_v_f32m1(&o[i * col + j], o_vec, vl);

      j += vl;
    }
  }
}

// mat_a: dim1 x dim2
// mat_b: dim2 x dim3
// o    : dim1 x dim3 (transposed dim3 x dim1)

// void matmul(float *mat_a, float *mat_b, float *o, int dim1, int dim2, int dim3, int transposed) {
//   size_t vlmax = vsetvlmax_e32m1();
//   vfloat32m1_t partial_sum;

//   for (int i = 0; i < dim1; i++) {
//     for (int j = 0; j < dim3;) {
//       int vl = vlmax;
//       if (j + vlmax > dim3) vl = vsetvl_e32m1(dim3 - j);
//       
//       partial_sum = vfmv_v_f_f32m1(0, vl);

//       for (int k = 0; k < dim2; k++) {
//         vfloat32m1_t b_vec = vle32_v_f32m1(&mat_b[k * dim3 + j], vl);
//         partial_sum = vfmacc_vf_f32m1(partial_sum, mat_a[i * dim2 + k], b_vec, vl);
//       }

//       if (transposed == 0) vse32_v_f32m1(&o[i * dim3 + j], partial_sum, vl);
//       else vsse32_v_f32m1(&o[j * dim1 + i], dim1 * 4, partial_sum, vl);
//       j += vl;
//     }
//   }
// }


void matmul(float *mat_a, float *mat_b, float *o, int dim1, int dim2, int dim3, int transposed) {
  size_t vlmax = vsetvlmax_e32m1();
  vfloat32m1_t partial_sum_1, partial_sum_2, partial_sum_3, partial_sum_4,
               partial_sum_5, partial_sum_6, partial_sum_7, partial_sum_8,
               partial_sum_9, partial_sum_10, partial_sum_11, partial_sum_12,
               partial_sum_13, partial_sum_14, partial_sum_15, partial_sum_16;

  for (int j = 0; j < dim3;) {
    int vl = vlmax;
    if (j + vlmax > dim3) vl = vsetvl_e32m1(dim3 - j);

    for (int i = 0; i < dim1; i = i + 16) {
      partial_sum_1 = vfmv_v_f_f32m1(0, vl);
      partial_sum_2 = vfmv_v_f_f32m1(0, vl);
      partial_sum_3 = vfmv_v_f_f32m1(0, vl);
      partial_sum_4 = vfmv_v_f_f32m1(0, vl);
      partial_sum_5 = vfmv_v_f_f32m1(0, vl);
      partial_sum_6 = vfmv_v_f_f32m1(0, vl);
      partial_sum_7 = vfmv_v_f_f32m1(0, vl);
      partial_sum_8 = vfmv_v_f_f32m1(0, vl);
      partial_sum_9 = vfmv_v_f_f32m1(0, vl);
      partial_sum_10 = vfmv_v_f_f32m1(0, vl);
      partial_sum_11 = vfmv_v_f_f32m1(0, vl);
      partial_sum_12 = vfmv_v_f_f32m1(0, vl);
      partial_sum_13 = vfmv_v_f_f32m1(0, vl);
      partial_sum_14 = vfmv_v_f_f32m1(0, vl);
      partial_sum_15 = vfmv_v_f_f32m1(0, vl);
      partial_sum_16 = vfmv_v_f_f32m1(0, vl);

      for (int k = 0; k < dim2; k += 2) {
        vfloat32m1_t b_vec = vle32_v_f32m1(&mat_b[k * dim3 + j], vl);
        partial_sum_1  = vfmacc_vf_f32m1(partial_sum_1,  mat_a[i * dim2 + k],        b_vec, vl);
        partial_sum_2  = vfmacc_vf_f32m1(partial_sum_2,  mat_a[(i + 1) * dim2 + k],  b_vec, vl);
        vfloat32m1_t b_vec2 = vle32_v_f32m1(&mat_b[(k + 1) * dim3 + j], vl);
        partial_sum_3  = vfmacc_vf_f32m1(partial_sum_3,  mat_a[(i + 2) * dim2 + k],  b_vec, vl);
        partial_sum_4  = vfmacc_vf_f32m1(partial_sum_4,  mat_a[(i + 3) * dim2 + k],  b_vec, vl);
        partial_sum_5  = vfmacc_vf_f32m1(partial_sum_5,  mat_a[(i + 4) * dim2 + k],  b_vec, vl);
        partial_sum_6  = vfmacc_vf_f32m1(partial_sum_6,  mat_a[(i + 5) * dim2 + k],  b_vec, vl);
        partial_sum_7  = vfmacc_vf_f32m1(partial_sum_7,  mat_a[(i + 6) * dim2 + k],  b_vec, vl);
        partial_sum_8  = vfmacc_vf_f32m1(partial_sum_8,  mat_a[(i + 7) * dim2 + k],  b_vec, vl);
        partial_sum_9  = vfmacc_vf_f32m1(partial_sum_9,  mat_a[(i + 8) * dim2 + k],  b_vec, vl);
        partial_sum_10 = vfmacc_vf_f32m1(partial_sum_10, mat_a[(i + 9) * dim2 + k],  b_vec, vl);
        partial_sum_11 = vfmacc_vf_f32m1(partial_sum_11, mat_a[(i + 10) * dim2 + k], b_vec, vl);
        partial_sum_12 = vfmacc_vf_f32m1(partial_sum_12, mat_a[(i + 11) * dim2 + k], b_vec, vl);
        partial_sum_13 = vfmacc_vf_f32m1(partial_sum_13, mat_a[(i + 12) * dim2 + k], b_vec, vl);
        partial_sum_14 = vfmacc_vf_f32m1(partial_sum_14, mat_a[(i + 13) * dim2 + k], b_vec, vl);
        partial_sum_15 = vfmacc_vf_f32m1(partial_sum_15, mat_a[(i + 14) * dim2 + k], b_vec, vl);
        partial_sum_16 = vfmacc_vf_f32m1(partial_sum_16, mat_a[(i + 15) * dim2 + k], b_vec, vl);

        partial_sum_1  = vfmacc_vf_f32m1(partial_sum_1,  mat_a[i * dim2 + k + 1],        b_vec2, vl);
        partial_sum_2  = vfmacc_vf_f32m1(partial_sum_2,  mat_a[(i + 1) * dim2 + k + 1],  b_vec2, vl);
        partial_sum_3  = vfmacc_vf_f32m1(partial_sum_3,  mat_a[(i + 2) * dim2 + k + 1],  b_vec2, vl);
        partial_sum_4  = vfmacc_vf_f32m1(partial_sum_4,  mat_a[(i + 3) * dim2 + k + 1],  b_vec2, vl);
        partial_sum_5  = vfmacc_vf_f32m1(partial_sum_5,  mat_a[(i + 4) * dim2 + k + 1],  b_vec2, vl);
        partial_sum_6  = vfmacc_vf_f32m1(partial_sum_6,  mat_a[(i + 5) * dim2 + k + 1],  b_vec2, vl);
        partial_sum_7  = vfmacc_vf_f32m1(partial_sum_7,  mat_a[(i + 6) * dim2 + k + 1],  b_vec2, vl);
        partial_sum_8  = vfmacc_vf_f32m1(partial_sum_8,  mat_a[(i + 7) * dim2 + k + 1],  b_vec2, vl);
        partial_sum_9  = vfmacc_vf_f32m1(partial_sum_9,  mat_a[(i + 8) * dim2 + k + 1],  b_vec2, vl);
        partial_sum_10 = vfmacc_vf_f32m1(partial_sum_10, mat_a[(i + 9) * dim2 + k + 1],  b_vec2, vl);
        partial_sum_11 = vfmacc_vf_f32m1(partial_sum_11, mat_a[(i + 10) * dim2 + k + 1], b_vec2, vl);
        partial_sum_12 = vfmacc_vf_f32m1(partial_sum_12, mat_a[(i + 11) * dim2 + k + 1], b_vec2, vl);
        partial_sum_13 = vfmacc_vf_f32m1(partial_sum_13, mat_a[(i + 12) * dim2 + k + 1], b_vec2, vl);
        partial_sum_14 = vfmacc_vf_f32m1(partial_sum_14, mat_a[(i + 13) * dim2 + k + 1], b_vec2, vl);
        partial_sum_15 = vfmacc_vf_f32m1(partial_sum_15, mat_a[(i + 14) * dim2 + k + 1], b_vec2, vl);
        partial_sum_16 = vfmacc_vf_f32m1(partial_sum_16, mat_a[(i + 15) * dim2 + k + 1], b_vec2, vl);
      }

      vse32_v_f32m1(&o[i * dim3 + j], partial_sum_1, vl);
      vse32_v_f32m1(&o[(i + 1) * dim3 + j], partial_sum_2, vl);
      vse32_v_f32m1(&o[(i + 2) * dim3 + j], partial_sum_3, vl);
      vse32_v_f32m1(&o[(i + 3) * dim3 + j], partial_sum_4, vl);
      vse32_v_f32m1(&o[(i + 4) * dim3 + j], partial_sum_5, vl);
      vse32_v_f32m1(&o[(i + 5) * dim3 + j], partial_sum_6, vl);
      vse32_v_f32m1(&o[(i + 6) * dim3 + j], partial_sum_7, vl);
      vse32_v_f32m1(&o[(i + 7) * dim3 + j], partial_sum_8, vl);
      vse32_v_f32m1(&o[(i + 8) * dim3 + j], partial_sum_9, vl);
      vse32_v_f32m1(&o[(i + 9) * dim3 + j], partial_sum_10, vl);
      vse32_v_f32m1(&o[(i + 10) * dim3 + j], partial_sum_11, vl);
      vse32_v_f32m1(&o[(i + 11) * dim3 + j], partial_sum_12, vl);
      vse32_v_f32m1(&o[(i + 12) * dim3 + j], partial_sum_13, vl);
      vse32_v_f32m1(&o[(i + 13) * dim3 + j], partial_sum_14, vl);
      vse32_v_f32m1(&o[(i + 14) * dim3 + j], partial_sum_15, vl);
      vse32_v_f32m1(&o[(i + 15) * dim3 + j], partial_sum_16, vl);
      // if (transposed == 0) { 
      //   vse32_v_f32m1(&o[i * dim3 + j], partial_sum_1, vl);
      //   vse32_v_f32m1(&o[(i + 1) * dim3 + j], partial_sum_2, vl);
      //   vse32_v_f32m1(&o[(i + 2) * dim3 + j], partial_sum_3, vl);
      //   vse32_v_f32m1(&o[(i + 3) * dim3 + j], partial_sum_4, vl);
      //   vse32_v_f32m1(&o[(i + 4) * dim3 + j], partial_sum_5, vl);
      //   vse32_v_f32m1(&o[(i + 5) * dim3 + j], partial_sum_6, vl);
      //   vse32_v_f32m1(&o[(i + 6) * dim3 + j], partial_sum_7, vl);
      //   vse32_v_f32m1(&o[(i + 7) * dim3 + j], partial_sum_8, vl);
      //   vse32_v_f32m1(&o[(i + 8) * dim3 + j], partial_sum_9, vl);
      //   vse32_v_f32m1(&o[(i + 9) * dim3 + j], partial_sum_10, vl);
      //   vse32_v_f32m1(&o[(i + 10) * dim3 + j], partial_sum_11, vl);
      //   vse32_v_f32m1(&o[(i + 11) * dim3 + j], partial_sum_12, vl);
      //   vse32_v_f32m1(&o[(i + 12) * dim3 + j], partial_sum_13, vl);
      //   vse32_v_f32m1(&o[(i + 13) * dim3 + j], partial_sum_14, vl);
      //   vse32_v_f32m1(&o[(i + 14) * dim3 + j], partial_sum_15, vl);
      //   vse32_v_f32m1(&o[(i + 15) * dim3 + j], partial_sum_16, vl);
      // } else {
      //   vsse32_v_f32m1(&o[j * dim1 + i], dim1 * 4, partial_sum_1, vl);
      //   vsse32_v_f32m1(&o[j * dim1 + i + 1], dim1 * 4, partial_sum_2, vl);
      //   vsse32_v_f32m1(&o[j * dim1 + i + 2], dim1 * 4, partial_sum_3, vl);
      //   vsse32_v_f32m1(&o[j * dim1 + i + 3], dim1 * 4, partial_sum_4, vl);
      // }
    }
    j += vl;
  }
}


// void matmul(float *mat_a, float *mat_b, float *o, int dim1, int dim2, int dim3, int transposed) {
//   size_t vlmax = vsetvlmax_e32m1();
//   vfloat32m1_t partial_sum_1, partial_sum_2, partial_sum_3, partial_sum_4,
//                partial_sum_5, partial_sum_6, partial_sum_7, partial_sum_8,
//                partial_sum_9, partial_sum_10, partial_sum_11, partial_sum_12,
//                partial_sum_13, partial_sum_14, partial_sum_15, partial_sum_16;


//   for (int j = 0; j < dim3;) {
//       int vl = vlmax;
//       if (j + vlmax > dim3) vl = vsetvl_e32m1(dim3 - j);
//     for (int i = 0; i < dim1; i += 16) {
//       partial_sum_1 = vfmv_v_f_f32m1(0, vl);
//       partial_sum_2 = vfmv_v_f_f32m1(0, vl);
//       partial_sum_3 = vfmv_v_f_f32m1(0, vl);
//       partial_sum_4 = vfmv_v_f_f32m1(0, vl);
//       partial_sum_5 = vfmv_v_f_f32m1(0, vl);
//       partial_sum_6 = vfmv_v_f_f32m1(0, vl);
//       partial_sum_7 = vfmv_v_f_f32m1(0, vl);
//       partial_sum_8 = vfmv_v_f_f32m1(0, vl);
//       partial_sum_9 = vfmv_v_f_f32m1(0, vl);
//       partial_sum_10 = vfmv_v_f_f32m1(0, vl);
//       partial_sum_11 = vfmv_v_f_f32m1(0, vl);
//       partial_sum_12 = vfmv_v_f_f32m1(0, vl);
//       partial_sum_13 = vfmv_v_f_f32m1(0, vl);
//       partial_sum_14 = vfmv_v_f_f32m1(0, vl);
//       partial_sum_15 = vfmv_v_f_f32m1(0, vl);
//       partial_sum_16 = vfmv_v_f_f32m1(0, vl);

//       float t0, t1, t2, t3, t4, t5, t6, t7, t8, t9, t10, t11, t12, t13, t14, t15;
//       float *a = mat_a + i * dim2;
//       t0 = *a, a += dim2;
//       t1 = *a, a += dim2;
//       t2 = *a, a += dim2;
//       t3 = *a, a += dim2;
//       t4 = *a, a += dim2;
//       t5 = *a, a += dim2;
//       t6 = *a, a += dim2;
//       t7 = *a, a += dim2;
//       t8 = *a, a += dim2;
//       t9 = *a, a += dim2;
//       t10 = *a, a += dim2;
//       t11 = *a, a += dim2;
//       t12 = *a, a += dim2;
//       t13 = *a, a += dim2;
//       t14 = *a, a += dim2;
//       t15 = *a;

//       float *b = mat_b;
//       vfloat32m1_t b_vec1 = vle32_v_f32m1(b, vl);
//       b += dim3;

//       int k = 0;
//       while (k != dim3) {
//         a = mat_a + i * dim2 + ++k;

//         partial_sum_1  = vfmacc_vf_f32m1(partial_sum_1,  t0, b_vec1, vl);
//         t0 = *a, a += dim2;

//         vfloat32m1_t b_vec2 = vle32_v_f32m1(b, vl);
//         b += dim3;

//         partial_sum_2  = vfmacc_vf_f32m1(partial_sum_2, t1,  b_vec1, vl);
//         t1 = *a, a += dim2;
//         partial_sum_3  = vfmacc_vf_f32m1(partial_sum_3, t2,  b_vec1, vl);
//         t2 = *a, a += dim2;
//         partial_sum_4  = vfmacc_vf_f32m1(partial_sum_4, t3,  b_vec1, vl);
//         t3 = *a, a += dim2;
//         partial_sum_5  = vfmacc_vf_f32m1(partial_sum_5, t4,  b_vec1, vl);
//         t4 = *a, a += dim2;
//         partial_sum_6  = vfmacc_vf_f32m1(partial_sum_6, t5,  b_vec1, vl);
//         t5 = *a, a += dim2;
//         partial_sum_7  = vfmacc_vf_f32m1(partial_sum_7, t6,  b_vec1, vl);
//         t6 = *a, a += dim2;
//         partial_sum_8  = vfmacc_vf_f32m1(partial_sum_8, t7,  b_vec1, vl);
//         t7 = *a, a += dim2;
//         partial_sum_9  = vfmacc_vf_f32m1(partial_sum_9, t8,  b_vec1, vl);
//         t8 = *a, a += dim2;
//         partial_sum_10 = vfmacc_vf_f32m1(partial_sum_10, t9,  b_vec1, vl);
//         t9 = *a, a += dim2;
//         partial_sum_11 = vfmacc_vf_f32m1(partial_sum_11, t10, b_vec1, vl);
//         t10 = *a, a += dim2;
//         partial_sum_12 = vfmacc_vf_f32m1(partial_sum_12, t11, b_vec1, vl);
//         t11 = *a, a += dim2;
//         partial_sum_13 = vfmacc_vf_f32m1(partial_sum_13, t12, b_vec1, vl);
//         t12 = *a, a += dim2;
//         partial_sum_14 = vfmacc_vf_f32m1(partial_sum_14, t13, b_vec1, vl);
//         t13 = *a, a += dim2;
//         partial_sum_15 = vfmacc_vf_f32m1(partial_sum_15, t14, b_vec1, vl);
//         t14 = *a, a += dim2;
//         partial_sum_16 = vfmacc_vf_f32m1(partial_sum_16, t15, b_vec1, vl);
//         t15 = *a;

//         a = mat_a + i * dim2 + ++k;
//         if (k == dim3) break;


//         partial_sum_1  = vfmacc_vf_f32m1(partial_sum_1,  t0, b_vec2, vl);
//         t0 = *a, a += dim2;

//         b_vec1 = vle32_v_f32m1(b, vl);
//         b += dim3;

//         partial_sum_2  = vfmacc_vf_f32m1(partial_sum_2, t1,  b_vec2, vl);
//         t1 = *a, a += dim2;
//         partial_sum_3  = vfmacc_vf_f32m1(partial_sum_3, t2,  b_vec2, vl);
//         t2 = *a, a += dim2;
//         partial_sum_4  = vfmacc_vf_f32m1(partial_sum_4, t3,  b_vec2, vl);
//         t3 = *a, a += dim2;
//         partial_sum_5  = vfmacc_vf_f32m1(partial_sum_5, t4,  b_vec2, vl);
//         t4 = *a, a += dim2;
//         partial_sum_6  = vfmacc_vf_f32m1(partial_sum_6, t5,  b_vec2, vl);
//         t5 = *a, a += dim2;
//         partial_sum_7  = vfmacc_vf_f32m1(partial_sum_7, t6,  b_vec2, vl);
//         t6 = *a, a += dim2;
//         partial_sum_8  = vfmacc_vf_f32m1(partial_sum_8, t7,  b_vec2, vl);
//         t7 = *a, a += dim2;
//         partial_sum_9  = vfmacc_vf_f32m1(partial_sum_9, t8,  b_vec2, vl);
//         t8 = *a, a += dim2;
//         partial_sum_10 = vfmacc_vf_f32m1(partial_sum_10, t9,  b_vec2, vl);
//         t9 = *a, a += dim2;
//         partial_sum_11 = vfmacc_vf_f32m1(partial_sum_11, t10, b_vec2, vl);
//         t10 = *a, a += dim2;
//         partial_sum_12 = vfmacc_vf_f32m1(partial_sum_12, t11, b_vec2, vl);
//         t11 = *a, a += dim2;
//         partial_sum_13 = vfmacc_vf_f32m1(partial_sum_13, t12, b_vec2, vl);
//         t12 = *a, a += dim2;
//         partial_sum_14 = vfmacc_vf_f32m1(partial_sum_14, t13, b_vec2, vl);
//         t13 = *a, a += dim2;
//         partial_sum_15 = vfmacc_vf_f32m1(partial_sum_15, t14, b_vec2, vl);
//         t14 = *a, a += dim2;
//         partial_sum_16 = vfmacc_vf_f32m1(partial_sum_16, t15, b_vec2, vl);
//         t15 = *a;
//       }

//       if (transposed == 0) { 
//         vse32_v_f32m1(&o[i * dim3 + j], partial_sum_1, vl);
//         vse32_v_f32m1(&o[(i + 1) * dim3 + j], partial_sum_2, vl);
//         vse32_v_f32m1(&o[(i + 2) * dim3 + j], partial_sum_3, vl);
//         vse32_v_f32m1(&o[(i + 3) * dim3 + j], partial_sum_4, vl);
//         vse32_v_f32m1(&o[(i + 4) * dim3 + j], partial_sum_5, vl);
//         vse32_v_f32m1(&o[(i + 5) * dim3 + j], partial_sum_6, vl);
//         vse32_v_f32m1(&o[(i + 6) * dim3 + j], partial_sum_7, vl);
//         vse32_v_f32m1(&o[(i + 7) * dim3 + j], partial_sum_8, vl);
//         vse32_v_f32m1(&o[(i + 8) * dim3 + j], partial_sum_9, vl);
//         vse32_v_f32m1(&o[(i + 9) * dim3 + j], partial_sum_10, vl);
//         vse32_v_f32m1(&o[(i + 10) * dim3 + j], partial_sum_11, vl);
//         vse32_v_f32m1(&o[(i + 11) * dim3 + j], partial_sum_12, vl);
//         vse32_v_f32m1(&o[(i + 12) * dim3 + j], partial_sum_13, vl);
//         vse32_v_f32m1(&o[(i + 13) * dim3 + j], partial_sum_14, vl);
//         vse32_v_f32m1(&o[(i + 14) * dim3 + j], partial_sum_15, vl);
//         vse32_v_f32m1(&o[(i + 15) * dim3 + j], partial_sum_16, vl);
//       } else {
//         vsse32_v_f32m1(&o[j * dim1 + i], dim1 * 4, partial_sum_1, vl);
//         vsse32_v_f32m1(&o[j * dim1 + i + 1], dim1 * 4, partial_sum_2, vl);
//         vsse32_v_f32m1(&o[j * dim1 + i + 2], dim1 * 4, partial_sum_3, vl);
//         vsse32_v_f32m1(&o[j * dim1 + i + 3], dim1 * 4, partial_sum_4, vl);
//       }
//     }
//     j += vl;
//   }
// }

// add bias matrix to the matmul result
void matmul_biased(float *mat_a, float *mat_b, float *o, float *bias, int dim1, int dim2, int dim3, int transposed) {
  size_t vlmax = vsetvlmax_e32m1();
  vfloat32m1_t partial_sum;

  for (int i = 0; i < dim1; i++) {
    for (int j = 0; j < dim3;) {
      int vl = vlmax;
      if (j + vlmax > dim3) vl = vsetvl_e32m1(dim3 - j);
      
      partial_sum = vfmv_v_f_f32m1(0, vl);

      for (int k = 0; k < dim2; k++) {
        vfloat32m1_t b_vec = vle32_v_f32m1(&mat_b[k * dim3 + j], vl);
        partial_sum = vfmacc_vf_f32m1(partial_sum, mat_a[i * dim2 + k], b_vec, vl);
      }

      // vfloat32m1_t bias_vec = vle32_v_f32m1(&bias[i * dim3 + j], vl);
      // partial_sum = vfadd_vv_f32m1(partial_sum, bias_vec, vl);
      vfloat32m1_t bias_vec = vle32_v_f32m1(&bias[j], vl);
      partial_sum = vfadd_vv_f32m1(partial_sum, bias_vec, vl);

      if (transposed == 0) vse32_v_f32m1(&o[i * dim3 + j], partial_sum, vl);
      else vsse32_v_f32m1(&o[j * dim1 + i], dim1 * 4, partial_sum, vl);
      j += vl;
    }
  }
}

// scale the matmul result
void matmul_scaled(float *mat_a, float *mat_b, float *o, float scale, int dim1, int dim2, int dim3, int transposed) {
  size_t vlmax = vsetvlmax_e32m1();
  vfloat32m1_t partial_sum;

  for (int i = 0; i < dim1; i++) {
    for (int j = 0; j < dim3;) {
      int vl = vlmax;
      if (j + vlmax > dim3) vl = vsetvl_e32m1(dim3 - j);
      
      partial_sum = vfmv_v_f_f32m1(0, vl);

      for (int k = 0; k < dim2; k++) {
        vfloat32m1_t b_vec = vle32_v_f32m1(&mat_b[k * dim3 + j], vl);
        partial_sum = vfmacc_vf_f32m1(partial_sum, mat_a[i * dim2 + k], b_vec, vl);
      }

      partial_sum = vfdiv_vf_f32m1(partial_sum, scale, vl);

      if (transposed == 0) vse32_v_f32m1(&o[i * dim3 + j], partial_sum, vl);
      else vsse32_v_f32m1(&o[j * dim1 + i], dim1 * 4, partial_sum, vl);
      j += vl;
    }
  }
}

void transpose(float *mat, float *o, int row, int col) {
  size_t vlmax = vsetvlmax_e32m1();
  for (int i = 0; i < row;) {
    int vl = vlmax;
    if (i + vlmax > row) vl = vsetvl_e32m1(row - i);
    
    for (int j = 0; j < col; j++) {
      vfloat32m1_t vec = vle32_v_f32m1(&mat[i * col + j], vl);
      vsse32_v_f32m1(&o[j * row + i], 4 * row, vec, vl);
    }

    i += vl;
  }
}
